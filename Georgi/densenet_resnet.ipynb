{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23321321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "#!pip install opencv-python \n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8045d1",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f9a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_images = '../data/ML/train/target_0'\n",
    "tumor_images = '../data/ML/train/target_1'\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5c540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_loaded = load_images_from_folder(healthy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0061dc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3618"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(healthy_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef65e63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_loaded[3000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ca5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_loaded = load_images_from_folder(tumor_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09dd98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tumor_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237589d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tumor_loaded[300].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "895344f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "root_dir = '/home/user/data/ML/train' # data root path\n",
    "classes_names = ['target_0', 'target_1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952b2b1",
   "metadata": {},
   "source": [
    "## Split in train/test/val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "952b68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs():\n",
    "    \n",
    "    for cls in classes_names:\n",
    "        os.makedirs(os.path.join(root_dir, 'train', cls))\n",
    "        os.makedirs(os.path.join(root_dir, 'val', cls))\n",
    "        os.makedirs(os.path.join(root_dir, 'test', cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c020f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.05\n",
    "val_ratio = 0.15\n",
    "\n",
    "\n",
    "all_healthy_filenames = os.listdir(os.path.join(root_dir,classes_names[0]))\n",
    "\n",
    "np.random.shuffle(all_healthy_filenames)\n",
    "train_healthy, val_healthy, test_healthy = np.split(np.array(all_healthy_filenames),\n",
    "                                                          [int(len(all_healthy_filenames)* (1 - (val_ratio + test_ratio))), \n",
    "                                                           int(len(all_healthy_filenames)* (1 - test_ratio))])\n",
    "\n",
    "\n",
    "train_healthy = [os.path.join(root_dir, classes_names[0], name) for name in train_healthy]\n",
    "val_healthy = [os.path.join(root_dir, classes_names[0],  name) for name in val_healthy]\n",
    "test_healthy = [os.path.join(root_dir, classes_names[0], name) for name in test_healthy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d924c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_healthy():\n",
    "    for name in train_healthy:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'train', classes_names[0]))\n",
    "    for name in test_healthy:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'test', classes_names[0]))\n",
    "    for name in val_healthy:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'val', classes_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e703d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  3618\n",
      "Training:  2894\n",
      "Validation:  543\n",
      "Testing:  181\n"
     ]
    }
   ],
   "source": [
    "print('Total images: ', len(all_healthy_filenames))\n",
    "print('Training: ', len(train_healthy))\n",
    "print('Validation: ', len(val_healthy))\n",
    "print('Testing: ', len(test_healthy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e4e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tumor_filenames = os.listdir(os.path.join(root_dir,classes_names[1]))\n",
    "\n",
    "np.random.shuffle(all_tumor_filenames)\n",
    "train_tumor, val_tumor, test_tumor = np.split(np.array(all_tumor_filenames),\n",
    "                                                          [int(len(all_tumor_filenames)* (1 - (val_ratio + test_ratio))), \n",
    "                                                           int(len(all_tumor_filenames)* (1 - test_ratio))])\n",
    "\n",
    "\n",
    "train_tumor = [os.path.join(root_dir, classes_names[1], name) for name in train_tumor]\n",
    "val_tumor = [os.path.join(root_dir, classes_names[1],  name) for name in val_tumor]\n",
    "test_tumor = [os.path.join(root_dir, classes_names[1], name) for name in test_tumor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c1dd50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  1382\n",
      "Training:  1105\n",
      "Validation:  207\n",
      "Testing:  70\n"
     ]
    }
   ],
   "source": [
    "print('Total images: ', len(all_tumor_filenames))\n",
    "print('Training: ', len(train_tumor))\n",
    "print('Validation: ', len(val_tumor))\n",
    "print('Testing: ', len(test_tumor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f76d1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_tumor():\n",
    "    for name in train_tumor:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'train', classes_names[1]))\n",
    "    for name in val_tumor:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'test', classes_names[1]))\n",
    "    for name in test_tumor:\n",
    "        shutil.copy(name, os.path.join(root_dir, 'val', classes_names[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0a3f595",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tumor_check = os.listdir(os.path.join(root_dir,'train', classes_names[1]))\n",
    "len(train_tumor_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d6e79",
   "metadata": {},
   "source": [
    "## Add augmentations (so far None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4449578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add augmentations\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "    ]),    \n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97dff1",
   "metadata": {},
   "source": [
    "## Create DataSet and DataLoader for Training and Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a2c30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/user/data/ML/train'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c250febe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset ImageFolder\n",
       "     Number of datapoints: 3999\n",
       "     Root location: /home/user/data/ML/train/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " 'val': Dataset ImageFolder\n",
       "     Number of datapoints: 613\n",
       "     Root location: /home/user/data/ML/train/val\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            )}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eb164c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79d01e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_0', 'target_1']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cf9704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 3999, 'val': 613}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "967aaa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46914fd",
   "metadata": {},
   "source": [
    "## Create DataSet and DataLoader for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61095a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "test_data_path = '/home/user/data/test'\n",
    "classes_names = ['target_0', 'target_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fb13f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets = {'test': datasets.ImageFolder(os.path.join(test_data_path), data_transforms['test'])}\n",
    "image_datasets['test'][3300][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "013f640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = {'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=1,\n",
    "                                             shuffle=True, num_workers=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7a1de22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdf2b0",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17b4abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_save, criterion, optimizer, scheduler, num_epochs=4):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.float()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(inputs).squeeze(1)\n",
    "                    #print(outputs.data, labels)\n",
    "                    loss = criterion(output, labels)\n",
    "                    preds = torch.sigmoid(output)\n",
    "                    thre = 0.5\n",
    "                    threshold = torch.tensor([0.5]).to(device)\n",
    "                    preds = (preds>threshold).float()\n",
    "                    #print(preds, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    \n",
    "    PATH = model_save\n",
    "    torch.save(best_model_wts, PATH)\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e211581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, saved_weights, criterion, optimizer, scheduler):\n",
    "    \n",
    "    model.load_state_dict(torch.load(saved_weights))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    predictions = []\n",
    "            \n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader['test']:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            labels = labels.float()\n",
    "            # calculate outputs by running images through the network\n",
    "            output = model(images).squeeze(1)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            loss = criterion(output, labels)\n",
    "            preds = torch.sigmoid(output)\n",
    "            predictions.append(preds)\n",
    "            thre = 0.5\n",
    "            threshold = torch.tensor([0.5]).to(device)\n",
    "            pred_binary = (preds>threshold).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (pred_binary == labels).sum().item()\n",
    "\n",
    "    print(total)\n",
    "    print('Accuracy of the network on the 6000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44def7",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba87af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af85a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.6207 Acc: 0.7059\n",
      "val Loss: 9.0782 Acc: 0.4976\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.6067 Acc: 0.7182\n",
      "val Loss: 0.3427 Acc: 0.8858\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.5978 Acc: 0.7229\n",
      "val Loss: 0.4550 Acc: 0.8858\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.5969 Acc: 0.7237\n",
      "val Loss: 0.4630 Acc: 0.8858\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.7237\n",
      "val Loss: 0.4704 Acc: 0.8858\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.7232\n",
      "val Loss: 0.4032 Acc: 0.8858\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.5928 Acc: 0.7237\n",
      "val Loss: 0.6857 Acc: 0.8858\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.5888 Acc: 0.7237\n",
      "val Loss: 0.5096 Acc: 0.8858\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.5865 Acc: 0.7237\n",
      "val Loss: 0.5328 Acc: 0.8858\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.5842 Acc: 0.7237\n",
      "val Loss: 0.6153 Acc: 0.8858\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.5806 Acc: 0.7237\n",
      "val Loss: 0.6899 Acc: 0.8858\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.5761 Acc: 0.7237\n",
      "val Loss: 0.8890 Acc: 0.8858\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.5686 Acc: 0.7237\n",
      "val Loss: 0.9713 Acc: 0.8858\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.5528 Acc: 0.7239\n",
      "val Loss: 0.8964 Acc: 0.8858\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.5264 Acc: 0.7237\n",
      "val Loss: 0.8637 Acc: 0.8858\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.5194 Acc: 0.7259\n",
      "val Loss: 1.3744 Acc: 0.8858\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.5128 Acc: 0.7312\n",
      "val Loss: 1.4293 Acc: 0.8858\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.5058 Acc: 0.7494\n",
      "val Loss: 0.7657 Acc: 0.8858\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.4977 Acc: 0.7692\n",
      "val Loss: 1.4521 Acc: 0.8858\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.4885 Acc: 0.7799\n",
      "val Loss: 1.6249 Acc: 0.8858\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.4768 Acc: 0.7917\n",
      "val Loss: 1.5715 Acc: 0.8858\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.4686 Acc: 0.7967\n",
      "val Loss: 1.6268 Acc: 0.8858\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.4671 Acc: 0.7982\n",
      "val Loss: 1.1828 Acc: 0.8858\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.4657 Acc: 0.7999\n",
      "val Loss: 1.1635 Acc: 0.8858\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.4644 Acc: 0.7999\n",
      "val Loss: 1.2632 Acc: 0.8858\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.4631 Acc: 0.8007\n",
      "val Loss: 1.4580 Acc: 0.8858\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.4617 Acc: 0.8035\n",
      "val Loss: 2.0095 Acc: 0.8858\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.4604 Acc: 0.8035\n",
      "val Loss: 1.2861 Acc: 0.8858\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.4595 Acc: 0.8052\n",
      "val Loss: 1.1942 Acc: 0.8858\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.4594 Acc: 0.8057\n",
      "val Loss: 1.4594 Acc: 0.8858\n",
      "\n",
      "Training complete in 38m 31s\n",
      "Best val Acc: 0.885808\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "model_ft = train_model(model_ft, \"resnet_best_model_weights.pth\", criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5eda7191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Accuracy of the network on the 6000 test images: 54 %\n"
     ]
    }
   ],
   "source": [
    "#test \n",
    "PATH = './resnet_best_model_weights.pth'\n",
    "\n",
    "predictions = test(model_ft, test_loader, PATH, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb68943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(data={'file_paths': file_paths, 'predictions': predictions})\n",
    "df_submission[\"file_paths\"] = df_submission[\"file_paths\"].apply(lambda x: x.replace(\"/home/user/data/test\",\"/data/challenges_data/test\"))\n",
    "\n",
    "df_submission.to_csv('df_submission.csv', index=False)\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5a772",
   "metadata": {},
   "source": [
    "## DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b59f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = models.densenet161(pretrained=True)\n",
    "num_ftrs = model_ft.classifier.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.classifier = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e83f44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.2673 Acc: 0.9057\n",
      "val Loss: 0.7758 Acc: 0.7031\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.2388 Acc: 0.9172\n",
      "val Loss: 0.3115 Acc: 0.9413\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.2085 Acc: 0.9237\n",
      "val Loss: 0.5749 Acc: 0.9005\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9595\n",
      "val Loss: 0.4590 Acc: 0.8989\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9780\n",
      "val Loss: 0.7910 Acc: 0.9070\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.0461 Acc: 0.9885\n",
      "val Loss: 0.2410 Acc: 0.9347\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.9942\n",
      "val Loss: 0.6810 Acc: 0.7292\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.0208 Acc: 0.9957\n",
      "val Loss: 0.3362 Acc: 0.9103\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.9970\n",
      "val Loss: 0.5129 Acc: 0.9038\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9977\n",
      "val Loss: 1.0931 Acc: 0.8940\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.9980\n",
      "val Loss: 0.5821 Acc: 0.8630\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9980\n",
      "val Loss: 0.3389 Acc: 0.9021\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.0082 Acc: 0.9980\n",
      "val Loss: 0.5440 Acc: 0.7977\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.0080 Acc: 0.9980\n",
      "val Loss: 1.4219 Acc: 0.6330\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.9980\n",
      "val Loss: 0.4994 Acc: 0.9103\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.9982\n",
      "val Loss: 0.5457 Acc: 0.9103\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.9982\n",
      "val Loss: 0.6022 Acc: 0.8646\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.9982\n",
      "val Loss: 0.4290 Acc: 0.8760\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.9982\n",
      "val Loss: 0.7210 Acc: 0.8515\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.9982\n",
      "val Loss: 0.4915 Acc: 0.9086\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9985\n",
      "val Loss: 0.5863 Acc: 0.8858\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9985\n",
      "val Loss: 0.5467 Acc: 0.8059\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9985\n",
      "val Loss: 0.4976 Acc: 0.9070\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9985\n",
      "val Loss: 0.2605 Acc: 0.9201\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.8364 Acc: 0.8336\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.2922 Acc: 0.9299\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.7632 Acc: 0.7749\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.3834 Acc: 0.8858\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.4079 Acc: 0.8450\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9985\n",
      "val Loss: 0.3458 Acc: 0.9331\n",
      "\n",
      "Training complete in 207m 19s\n",
      "Best val Acc: 0.941272\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "model_ft = train_model(model_ft, \"densenet_best_model_weights.pth\", criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "578b2500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Accuracy of the network on the 6000 test images: 68 %\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "PATH = './densenet_best_model_weights.pth'\n",
    "\n",
    "predictions = test(model_ft, test_loader, PATH, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(data={'file_paths': file_paths, 'predictions': predictions})\n",
    "df_submission[\"file_paths\"] = df_submission[\"file_paths\"].apply(lambda x: x.replace(\"/home/user/data/test\",\"/data/challenges_data/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('df_submission.csv', index=False)\n",
    "df_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elixenv",
   "language": "python",
   "name": "elixenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
