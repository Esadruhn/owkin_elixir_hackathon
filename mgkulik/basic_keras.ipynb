{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ba65bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "train_path = '/home/user/data/data/ML/train'\n",
    "validation_path = '/home/user/data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f0c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Checking the image sizes\n",
    "img = Image.open('/home/user/data/data/ML/train/target_1/tumor_0_9_9178.jpg')\n",
    "print(img.size)\n",
    "\n",
    "image_size = (180,180)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "eval_all, mdl_lst, aug_lst = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28920b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugment(object):\n",
    "    def __call__(self, image):        \n",
    "        # Random flips and grayscale with some stochasticity\n",
    "        img = self._random_apply(tf.image.flip_left_right, image, p=0.6)\n",
    "        img = self._random_apply(self._color_drop, img, p=0.9)\n",
    "        return img\n",
    "\n",
    "    def _color_drop(self, x):\n",
    "        image = tf.image.rgb_to_grayscale(x)\n",
    "        image = tf.tile(x, [1, 1, 1, 3])\n",
    "        return x\n",
    "    \n",
    "    def _random_apply(self, func, x, p):\n",
    "        return tf.cond(\n",
    "          tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "                  tf.cast(p, tf.float32)),\n",
    "          lambda: func(x),\n",
    "          lambda: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Using 4000 files for training.\n",
      "Found 6000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 09:11:54.900936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:54.915940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:54.917053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:54.918834: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-11 09:11:54.920873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:54.921881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:54.922849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:55.528036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:55.529060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:55.530010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-11 09:11:55.530952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=0,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=validation_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    shuffle=False,\n",
    "    seed=0,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed46c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(buffer_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b25be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_augmentation(input_shape):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "    return x, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66ab741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_basic_model(num_classes, x, inputs):\n",
    "    \n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56819f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_effB3_model(x, inputs):\n",
    "    \n",
    "    effB3_model = tf.keras.applications.EfficientNetB3(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=image_size + (3,),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "    effB3_model.trainable = False\n",
    "    \n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = effB3_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f62b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_effB7_model(x, inputs):\n",
    "    \n",
    "    effB7_model = tf.keras.applications.EfficientNetB7(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=image_size + (3,),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "    effB7_model.trainable = False\n",
    "    \n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = effB7_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf99176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vgg16_model(x, inputs):\n",
    "    \n",
    "    vgg16_model = tf.keras.applications.VGG16(\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=image_size + (3,),\n",
    "        include_top=False)\n",
    "    vgg16_model.trainable = False\n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = vgg16_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97090b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rn152v2_model(x, inputs):\n",
    "    \n",
    "    rn152v2_model = tf.keras.applications.ResNet152V2(\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=image_size + (3,),\n",
    "        include_top=False)\n",
    "    rn152v2_model.trainable = False\n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = rn152v2_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385803ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dn201_model(x, inputs):\n",
    "    \n",
    "    dn201_model = tf.keras.applications.DenseNet201(\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=image_size + (3,),\n",
    "        include_top=False)\n",
    "    dn201_model.trainable = False\n",
    "    x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = dn201_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bab09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(model, train_ds, test_ds, group, seq, mod_type):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    "    )\n",
    "    model.fit(train_ds, epochs=epochs)\n",
    "    eval_lst = [seq]+model.evaluate(train_ds)+model.evaluate(test_ds)+[group, mod_type]\n",
    "    \n",
    "    return model, eval_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff2c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_lst = [\"flip_rotation\", \"flip_color_rot\", \"flip_color_rot_zoom\", \n",
    "             \"flip_color_rot_zoom_gn\", \"flip_color_rot_zoom_cont\", \"flipHV_color_rot_zoom_cont\"]\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.RandomFlip(\"horizontal\"),\n",
    "                                 layers.RandomRotation(0.1)]))\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.Lambda(CustomAugment()),\n",
    "                                 layers.RandomRotation(0.1)]))\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.Lambda(CustomAugment()),\n",
    "                                 layers.RandomRotation(0.1),\n",
    "                                 layers.RandomZoom(0.1)]))\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.Lambda(CustomAugment()),\n",
    "                                 layers.RandomRotation(0.1),\n",
    "                                 layers.RandomZoom(0.1),\n",
    "                                 layers.GaussianNoise(0.1)]))\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.Lambda(CustomAugment()),\n",
    "                                 layers.RandomRotation(0.1),\n",
    "                                 layers.RandomZoom(0.1),\n",
    "                                 layers.GaussianNoise(0.1),\n",
    "                                 layers.RandomContrast(0.2)]))\n",
    "\n",
    "aug_lst.append(keras.Sequential([layers.RandomRotation(0.1),\n",
    "                                 layers.RandomZoom(0.1),\n",
    "                                 layers.GaussianNoise(0.1),\n",
    "                                 layers.RandomContrast(0.2),\n",
    "                                 layers.RandomFlip(\"horizontal_and_vertical\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e7167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_type = [\"basic\", \"effB3\", \"vgg16\", \"resnet152v2\", \"denseNet201\", \"effB7\"]\n",
    "mod_type = [\"effB3\", \"effB7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35c4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 15s 54ms/step - loss: 0.3710 - binary_accuracy: 0.8472 - auc_1: 0.8787\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2758 - binary_accuracy: 0.9032 - auc_1: 0.9221\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2547 - binary_accuracy: 0.9087 - auc_1: 0.9322\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2431 - binary_accuracy: 0.9120 - auc_1: 0.9375\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2337 - binary_accuracy: 0.9150 - auc_1: 0.9423\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2272 - binary_accuracy: 0.9187 - auc_1: 0.9463\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2205 - binary_accuracy: 0.9202 - auc_1: 0.9495\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2161 - binary_accuracy: 0.9237 - auc_1: 0.9513\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2116 - binary_accuracy: 0.9233 - auc_1: 0.9540\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.2070 - binary_accuracy: 0.9243 - auc_1: 0.9566\n",
      "125/125 [==============================] - 9s 53ms/step - loss: 0.2014 - binary_accuracy: 0.9287 - auc_1: 0.9598\n",
      "188/188 [==============================] - 10s 53ms/step - loss: 0.2766 - binary_accuracy: 0.8963 - auc_1: 0.9443\n",
      "\n",
      "Model: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_effB7_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25459/2880441557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_effB3_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"effB7\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_effB7_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"vgg16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vgg16_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_effB7_model' is not defined"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "starts_ends = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(aug_lst)):\n",
    "    for j in mod_type:\n",
    "        print(\"Model: \" + str(k))\n",
    "        data_augmentation = aug_lst[i]\n",
    "        aug_mod, aug_inputs = gen_augmentation(image_size + (3,))\n",
    "        if (j==\"basic\"):\n",
    "            model = make_basic_model(num_classes=2, x=aug_mod, inputs=aug_inputs)\n",
    "        elif (j==\"effB3\"):\n",
    "            model = make_effB3_model(x=aug_mod, inputs=aug_inputs)\n",
    "        elif (j==\"effB7\"):\n",
    "            model = make_effB7_model(x=aug_mod, inputs=aug_inputs)\n",
    "        elif (j==\"vgg16\"):\n",
    "            model = make_vgg16_model(x=aug_mod, inputs=aug_inputs)\n",
    "        elif (j==\"resnet152v2\"):\n",
    "            model = make_rn152v2_model(x=aug_mod, inputs=aug_inputs)\n",
    "        elif (j==\"denseNet201\"):\n",
    "            model = make_dn201_model(x=aug_mod, inputs=aug_inputs)\n",
    "        mdl, eval_lst = execute_model(model, train_ds, test_ds, group_lst[i], k, j)\n",
    "        mdl_lst.append(mdl)\n",
    "        eval_all.append(eval_lst)\n",
    "        k += 1\n",
    "        print()\n",
    "        \n",
    "tot_in_sec = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (tot_in_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"id\", \"train_loss\", \"train_bin_acc\", \"train_auc\", \"test_loss\", \"test_bin_acc\", \"test_auc\", \"group\", \"model\"]\n",
    "df_best_model = pd.DataFrame(eval_all,columns=cols)\n",
    "df_best_model = df_best_model.sort_values(\"test_auc\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee14f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = mdl_lst[df_best_model.loc[0, \"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = test_ds.file_paths\n",
    "\n",
    "def generate_sub_best_model(model, file_paths, save_sub, dtn):\n",
    "    predictions = np.array([])\n",
    "    labels =  np.array([])\n",
    "    for x, y in test_ds:\n",
    "        predictions = np.concatenate([predictions, model.predict(x).ravel()])\n",
    "        labels = np.concatenate([labels, y.numpy().ravel()])\n",
    "    df_solution = pd.DataFrame(data={'file_paths': file_paths, 'predictions': labels})\n",
    "    #df_solution.to_csv('df_solution.csv', index=False)\n",
    "    df_submission = pd.DataFrame(data={'file_paths': file_paths, 'predictions': predictions})\n",
    "    df_submission[\"file_paths\"] = df_submission[\"file_paths\"].apply(lambda x: \n",
    "                                                                    x.replace(\"/home/user/data/test\",\n",
    "                                                                              \"/data/challenges_data/test\"))\n",
    "    \n",
    "    if (save_sub):\n",
    "        df_submission.to_csv('df_submission_'+str(dtn)+'.csv', index=False)\n",
    "        model.save(\"best_model\"+dtn+\".h5\")\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "df_submission = generate_sub_best_model(mdl, file_paths, save_submissions, dtn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
